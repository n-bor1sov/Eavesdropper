{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/Documents/Nikita/linux/InnopolisUniversity/F24/PMLDL/Eavesdropper/eavesdropper/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained T5-Small model and tokenizer\n",
    "def load_model_tokenizer():\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize text using T5-Small\n",
    "def summarize_text(model, tokenizer, device, input_text, min_length=30, max_length=100):\n",
    "    input_ids = tokenizer(\"summarize: \" + input_text, return_tensors='pt').input_ids.to(device)\n",
    "    attention_mask = tokenizer(\"summarize: \" + input_text, return_tensors='pt').attention_mask.to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             num_beams=4, \n",
    "                             no_repeat_ngram_size=2, \n",
    "                             min_length=min_length, \n",
    "                             max_length=max_length)\n",
    "    \n",
    "    # Convert generated IDs to text\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "our Q2 sales are up by 15% compared to last year, with a total revenue of $1.2 million . our product team is working on some exciting updates that will further appeal to our target audience.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main function\n",
    "def main():\n",
    "    model, tokenizer, device = load_model_tokenizer()\n",
    "    \n",
    "    # Example input text for summarization\n",
    "    input_text = f\"\"\"\n",
    "    Your task to summarize the transcribtion of the online meeting. Separate the dialog on logical parts and describe opinion of each speaker related to this part.\n",
    "    Here the transcribtion:\n",
    "    \n",
    "    {test_transribtion}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Summarize the input text\n",
    "    summary = summarize_text(model, tokenizer, device, input_text)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SPEAKER_00]: And today, as Jim proved to us, without talking about artificial intelligence and large language models, I typically say artificial intelligence is autocorrect on steroids because all a large language model does is it predicts what's the most likely next word that you're going to use and then it extrapolates from there.\n",
      "\n",
      "\n",
      "[SPEAKER_00]: So not really very intelligent.\n",
      "\n",
      "\n",
      "[SPEAKER_00]: Obviously, the impact that it has on our lives and on the reality we live in is significant.\n",
      "\n",
      "\n",
      "[SPEAKER_00]: Do you think we will see LLM written code that is submitted to you as a progress?\n",
      "\n",
      "\n",
      "[SPEAKER_01]: I'm convinced it's going to happen, yes.\n",
      "\n",
      "\n",
      "[SPEAKER_01]: And it may well be happening already, maybe on a smaller scale where people really use it more as a help in writing code.\n",
      "\n",
      "\n",
      "[SPEAKER_01]: It's clearly something where automation has always helped people write code.\n",
      "\n",
      "\n",
      "[SPEAKER_01]: I mean, this is not anything new at all.\n",
      "\n",
      "\n",
      "[SPEAKER_01]: We don't write machine code anymore.\n",
      "\n",
      "10\n",
      "\n",
      "[SPEAKER_01]: We don't even write assembler.\n",
      "\n",
      "11\n",
      "\n",
      "[SPEAKER_01]: And now we're moving on from C to Rust.\n",
      "\n",
      "12\n",
      "\n",
      "[SPEAKER_01]: So I don't see this as something\n",
      "\n",
      "13\n",
      "\n",
      "[SPEAKER_01]: as revolutionary as all the news talk is every day about AI.\n",
      "\n",
      "14\n",
      "\n",
      "[SPEAKER_01]: It's not an area I obviously work with.\n",
      "\n",
      "15\n",
      "\n",
      "[SPEAKER_01]: I'm still very low level.\n",
      "\n",
      "16\n",
      "\n",
      "[SPEAKER_01]: I got into kernels because I love the low level hardware details.\n",
      "\n",
      "17\n",
      "\n",
      "[SPEAKER_01]: why I'm still there.\n",
      "\n",
      "18\n",
      "\n",
      "[SPEAKER_00]: But so you say you expect this can help people write code, this can help people get started, but then if we look at the previous conversation and the challenges around code reviews and maintaining, so do you think that large language models will get to the point that they can help us review code, that they can help maintain subsistence?\n",
      "\n",
      "19\n",
      "\n",
      "[SPEAKER_01]: I hope, I hope because that's certainly one of the areas where\n",
      "\n",
      "20\n",
      "\n",
      "[SPEAKER_01]: which I see them really being able to shine to find the obvious stupid bugs, because I mean, how many of people here are actually programmers in this room?\n",
      "\n",
      "21\n",
      "\n",
      "[SPEAKER_01]: So a fair number.\n",
      "\n",
      "22\n",
      "\n",
      "[SPEAKER_01]: A lot of the bugs I've read, right, a lot of the bugs I see other people write, they're not like subtle bugs.\n",
      "\n",
      "23\n",
      "\n",
      "[SPEAKER_01]: A lot of them are just the stupid bugs that you did not think about, and you don't need\n",
      "\n",
      "24\n",
      "\n",
      "[SPEAKER_01]: any kind of higher intelligence to find them.\n",
      "\n",
      "25\n",
      "\n",
      "[SPEAKER_01]: But having tools that warn, I mean, we have compilers that warn about the really obvious ones, but having LLMs that warn about slightly more subtle cases where it may just say, this pattern does not look like the regular pattern.\n",
      "\n",
      "26\n",
      "\n",
      "[SPEAKER_01]: Are you sure this is what you mean?\n",
      "\n",
      "27\n",
      "\n",
      "[SPEAKER_01]: And the answer may be, no, that was not at all what I meant.\n",
      "\n",
      "28\n",
      "\n",
      "[SPEAKER_01]: You found an obvious bug.\n",
      "\n",
      "29\n",
      "\n",
      "[SPEAKER_01]: Thank you very much.\n",
      "\n",
      "30\n",
      "\n",
      "[SPEAKER_01]: So I do think that LLMs are going to be a big, you call them disparagingly, like auto-corrects on steroids.\n",
      "\n",
      "31\n",
      "\n",
      "[SPEAKER_01]: And I actually think that they're way more than that and how most people work is we all are.\n",
      "\n",
      "32\n",
      "\n",
      "[SPEAKER_01]: auto-corrects on steroids to some degree.\n",
      "\n",
      "33\n",
      "\n",
      "[SPEAKER_01]: And I see that as a tool that can help us be better at what we do.\n",
      "\n",
      "34\n",
      "\n",
      "[SPEAKER_01]: But I've always been optimistic.\n",
      "\n",
      "35\n",
      "\n",
      "[SPEAKER_01]: The whole... Hopeful.\n",
      "\n",
      "36\n",
      "\n",
      "[SPEAKER_01]: Hopeful was the word you had.\n",
      "\n",
      "37\n",
      "\n",
      "[SPEAKER_00]: Yes, yes.\n",
      "\n",
      "38\n",
      "\n",
      "[SPEAKER_00]: Helpful, hopeful and humble.\n",
      "\n",
      "39\n",
      "\n",
      "[SPEAKER_01]: Hopeful and humble, that's my middle name.\n",
      "\n",
      "40\n",
      "\n",
      "[SPEAKER_01]: But...\n",
      "\n",
      "41\n",
      "\n",
      "[SPEAKER_01]: On the other hand, I mean, I have been so optimistic that 32 years ago I was stupid enough to think that you can write a better kernel than anybody else.\n",
      "\n",
      "42\n",
      "\n",
      "[SPEAKER_01]: So you have to kind of be a bit too optimistic at times to make a difference.\n",
      "\n",
      "43\n",
      "\n",
      "[SPEAKER_01]: So my approach to OOMs really has to be that, hey, this is wonderful.\n",
      "\n",
      "44\n",
      "\n",
      "[SPEAKER_01]: This is going to\n",
      "\n",
      "45\n",
      "\n",
      "[SPEAKER_00]: I love seeing the optimism.\n",
      "\n",
      "46\n",
      "\n",
      "[SPEAKER_00]: I don't necessarily share it.\n",
      "\n",
      "47\n",
      "\n",
      "[SPEAKER_01]: No, a lot of people disagree with it.\n",
      "\n",
      "48\n",
      "\n",
      "[SPEAKER_00]: But one of the things that I worry about in all this is we see the hallucinations.\n",
      "\n",
      "49\n",
      "\n",
      "[SPEAKER_00]: We see, and that's a technical term for LLMs, they do hallucinate and they do makeup stuff.\n",
      "\n",
      "50\n",
      "\n",
      "[SPEAKER_00]: And so the more they are being put into the position where they will automatically do things without an actual human being there to catch them, the more this becomes scary.\n",
      "\n",
      "51\n",
      "\n",
      "[SPEAKER_00]: not as curious and they will rule the world and not in the sci-fi sense, but in the so many bugs that will happen and that will affect our lives or our code.\n",
      "\n",
      "52\n",
      "\n",
      "[SPEAKER_01]: Well, I see the bugs that happen without them every day, so that may be why I'm not so worried.\n",
      "\n",
      "53\n",
      "\n",
      "[SPEAKER_01]: I think we're doing those just fine on our own.\n",
      "\n",
      "54\n",
      "\n",
      "[SPEAKER_00]: And I don't think I can end the AI topic on a better highlight, so let's go from there to...\n",
      "\n",
      "\n",
      "Summary:\n",
      "[SPEAKER_00]: I've always been optimistic about the future of large language models . \"i see the bugs that happen without them every day, so that may be why i'm not so worried,\" specker says if he's correcting a bug, \"we're doing those just fine on our own\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load pre-trained T5-Base model and tokenizer\n",
    "def load_model_tokenizer():\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "# Function to summarize text using T5-Base\n",
    "def summarize_text(model, tokenizer, device, input_text, min_length=30, max_length=500):\n",
    "    input_ids = tokenizer(\"summarize: \" + input_text, return_tensors='pt').input_ids.to(device)\n",
    "    attention_mask = tokenizer(\"summarize: \" + input_text, return_tensors='pt').attention_mask.to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             num_beams=4, \n",
    "                             no_repeat_ngram_size=2, \n",
    "                             min_length=min_length, \n",
    "                             max_length=max_length)\n",
    "    \n",
    "    # Convert generated IDs to text\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    model, tokenizer, device = load_model_tokenizer()\n",
    "    cleaned_text = ''\n",
    "    with open(\"../data/Sample.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "        text_without_timestamps = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n",
    "        cleaned_text = re.sub(r'^\\d$\\n', '', text_without_timestamps, flags=re.MULTILINE)\n",
    "    print(cleaned_text)\n",
    "    # Example input text for summarization\n",
    "    input_text = f\"\"\"\n",
    "    Your task to summarize the transcribtion of the online meeting. Separate the dialog on logical parts and describe what each specker said in term of this theme.\n",
    "    Here the transcribtion:\n",
    "    {cleaned_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Summarize the input text\n",
    "    summary = summarize_text(model, tokenizer, device, input_text)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from /home/nick/.cache/huggingface/hub/models--bartowski--gemma-2-2b-it-GGUF/snapshots/855f67caed130e1befc571b52bd181be2e858883/./gemma-2-2b-it-Q6_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llama_model_loader: - type q6_K:  182 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 249\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 2.13 GiB (7.00 BPW) \n",
      "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2182.19 MiB\n",
      "............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    52.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   52.00 MiB, K (f16):   26.00 MiB, V (f16):   26.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '18', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"bartowski/gemma-2-2b-it-GGUF\",\n",
    "\tfilename=\"gemma-2-2b-it-Q6_K_L.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    \"\"\"\n",
    "    Removes lines containing only numbers from the input text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with number-only lines removed.\n",
    "    \"\"\"\n",
    "    # Split the input text into lines\n",
    "    lines = input_text.split('\\n')\n",
    "    \n",
    "    # Filter out lines that contain only numbers (possibly with leading/trailing whitespace)\n",
    "    cleaned_lines = [line for line in lines if not re.match(r'^\\s*\\d+\\s*$', line)]\n",
    "    \n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start_of_turn>user\\nOnline Meeting Summary & Insights Assistant\\nTask:\\nAnalyze Discussion Structure: Identify and label key topics/subtopics discussed during the meeting.\\nSpeaker-wise Subtopic Summaries: For each subtopic, provide a brief summary (1-2 sentences) of the key points mentioned by each relevant speaker.\\nMeeting Conclusion & Key Takeaways:\\nShort Conclusion (2-3 sentences): Outline the overall discussion and meeting outcome.\\nEmphasized Key Points (bullet points): Highlight the most crucial decisions, actions, or agreements from the meeting.\\nInput:\\n\\nMeeting Transcript: \\n\\n\\n[SPEAKER_00]: And today, as Jim proved to us, without talking about artificial intelligence and large language models, I typically say artificial intelligence is autocorrect on steroids because all a large language model does is it predicts what's the most likely next word that you're going to use and then it extrapolates from there.\\n\\n\\n[SPEAKER_00]: So not really very intelligent.\\n\\n\\n[SPEAKER_00]: Obviously, the impact that it has on our lives and on the reality we live in is significant.\\n\\n\\n[SPEAKER_00]: Do you think we will see LLM written code that is submitted to you as a progress?\\n\\n\\n[SPEAKER_01]: I'm convinced it's going to happen, yes.\\n\\n\\n[SPEAKER_01]: And it may well be happening already, maybe on a smaller scale where people really use it more as a help in writing code.\\n\\n\\n[SPEAKER_01]: It's clearly something where automation has always helped people write code.\\n\\n\\n[SPEAKER_01]: I mean, this is not anything new at all.\\n\\n\\n[SPEAKER_01]: We don't write machine code anymore.\\n\\n\\n[SPEAKER_01]: We don't even write assembler.\\n\\n\\n[SPEAKER_01]: And now we're moving on from C to Rust.\\n\\n\\n[SPEAKER_01]: So I don't see this as something\\n\\n\\n[SPEAKER_01]: as revolutionary as all the news talk is every day about AI.\\n\\n\\n[SPEAKER_01]: It's not an area I obviously work with.\\n\\n\\n[SPEAKER_01]: I'm still very low level.\\n\\n\\n[SPEAKER_01]: I got into kernels because I love the low level hardware details.\\n\\n\\n[SPEAKER_01]: why I'm still there.\\n\\n\\n[SPEAKER_00]: But so you say you expect this can help people write code, this can help people get started, but then if we look at the previous conversation and the challenges around code reviews and maintaining, so do you think that large language models will get to the point that they can help us review code, that they can help maintain subsistence?\\n\\n\\n[SPEAKER_01]: I hope, I hope because that's certainly one of the areas where\\n\\n\\n[SPEAKER_01]: which I see them really being able to shine to find the obvious stupid bugs, because I mean, how many of people here are actually programmers in this room?\\n\\n\\n[SPEAKER_01]: So a fair number.\\n\\n\\n[SPEAKER_01]: A lot of the bugs I've read, right, a lot of the bugs I see other people write, they're not like subtle bugs.\\n\\n\\n[SPEAKER_01]: A lot of them are just the stupid bugs that you did not think about, and you don't need\\n\\n\\n[SPEAKER_01]: any kind of higher intelligence to find them.\\n\\n\\n[SPEAKER_01]: But having tools that warn, I mean, we have compilers that warn about the really obvious ones, but having LLMs that warn about slightly more subtle cases where it may just say, this pattern does not look like the regular pattern.\\n\\n\\n[SPEAKER_01]: Are you sure this is what you mean?\\n\\n\\n[SPEAKER_01]: And the answer may be, no, that was not at all what I meant.\\n\\n\\n[SPEAKER_01]: You found an obvious bug.\\n\\n\\n[SPEAKER_01]: Thank you very much.\\n\\n\\n[SPEAKER_01]: So I do think that LLMs are going to be a big, you call them disparagingly, like auto-corrects on steroids.\\n\\n\\n[SPEAKER_01]: And I actually think that they're way more than that and how most people work is we all are.\\n\\n\\n[SPEAKER_01]: auto-corrects on steroids to some degree.\\n\\n\\n[SPEAKER_01]: And I see that as a tool that can help us be better at what we do.\\n\\n\\n[SPEAKER_01]: But I've always been optimistic.\\n\\n\\n[SPEAKER_01]: The whole... Hopeful.\\n\\n\\n[SPEAKER_01]: Hopeful was the word you had.\\n\\n\\n[SPEAKER_00]: Yes, yes.\\n\\n\\n[SPEAKER_00]: Helpful, hopeful and humble.\\n\\n\\n[SPEAKER_01]: Hopeful and humble, that's my middle name.\\n\\n\\n[SPEAKER_01]: But...\\n\\n\\n[SPEAKER_01]: On the other hand, I mean, I have been so optimistic that 32 years ago I was stupid enough to think that you can write a better kernel than anybody else.\\n\\n\\n[SPEAKER_01]: So you have to kind of be a bit too optimistic at times to make a difference.\\n\\n\\n[SPEAKER_01]: So my approach to OOMs really has to be that, hey, this is wonderful.\\n\\n\\n[SPEAKER_01]: This is going to\\n\\n\\n[SPEAKER_00]: I love seeing the optimism.\\n\\n\\n[SPEAKER_00]: I don't necessarily share it.\\n\\n\\n[SPEAKER_01]: No, a lot of people disagree with it.\\n\\n\\n[SPEAKER_00]: But one of the things that I worry about in all this is we see the hallucinations.\\n\\n\\n[SPEAKER_00]: We see, and that's a technical term for LLMs, they do hallucinate and they do makeup stuff.\\n\\n\\n[SPEAKER_00]: And so the more they are being put into the position where they will automatically do things without an actual human being there to catch them, the more this becomes scary.\\n\\n\\n[SPEAKER_00]: not as curious and they will rule the world and not in the sci-fi sense, but in the so many bugs that will happen and that will affect our lives or our code.\\n\\n\\n[SPEAKER_01]: Well, I see the bugs that happen without them every day, so that may be why I'm not so worried.\\n\\n\\n[SPEAKER_01]: I think we're doing those just fine on our own.\\n\\n\\n[SPEAKER_00]: And I don't think I can end the AI topic on a better highlight, so let's go from there to...\\n\\n\\n\\nDesired Output Format:\\n\\n**Topic 1: [ Brief Topic Description ]**\\n* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\\n* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\\n*...\\n\\n**Topic 2: [ Brief Topic Description ]**\\n* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\\n* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\\n*...\\n\\n**Conclusion:**\\n* Brief summary of the overall discussion and meeting outcome (2-3 sentences)\\n\\n**Key Takeaways:**\\n* • Crucial Decision/Action 1\\n* • Crucial Decision/Action 2\\n* •...\\n\\nAnswer:<end_of_turn><eos>\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "\n",
    "SUMMARIZE_PROMPT = \"\"\"Online Meeting Summary & Insights Assistant\n",
    "Task:\n",
    "Analyze Discussion Structure: Identify and label key topics/subtopics discussed during the meeting.\n",
    "Speaker-wise Subtopic Summaries: For each subtopic, provide a brief summary (1-2 sentences) of the key points mentioned by each relevant speaker.\n",
    "Meeting Conclusion & Key Takeaways:\n",
    "Short Conclusion (2-3 sentences): Outline the overall discussion and meeting outcome.\n",
    "Emphasized Key Points (bullet points): Highlight the most crucial decisions, actions, or agreements from the meeting.\n",
    "Input:\n",
    "\n",
    "Meeting Transcript: \n",
    "\n",
    "{script}\n",
    "\n",
    "Desired Output Format:\n",
    "\n",
    "**Topic 1: [ Brief Topic Description ]**\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "*...\n",
    "\n",
    "**Topic 2: [ Brief Topic Description ]**\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "*...\n",
    "\n",
    "**Conclusion:**\n",
    "* Brief summary of the overall discussion and meeting outcome (2-3 sentences)\n",
    "\n",
    "**Key Takeaways:**\n",
    "* • Crucial Decision/Action 1\n",
    "* • Crucial Decision/Action 2\n",
    "* •...\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "cleaned_text = ''\n",
    "with open(\"../data/Sample.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "    text_without_timestamps = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n",
    "    cleaned_text = re.sub(r'^\\d$\\n', '', text_without_timestamps, flags=re.MULTILINE)\n",
    "    cleaned_text = clean_text(cleaned_text)\n",
    "\n",
    "final_prompt = USER_CHAT_TEMPLATE.format(\n",
    "        prompt=SUMMARIZE_PROMPT.format(script=cleaned_text)\n",
    "    )\n",
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (1618) exceed context window of 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m complition \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_prompt\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m answer \u001b[38;5;241m=\u001b[39m complition[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Nikita/linux/InnopolisUniversity/F24/PMLDL/Eavesdropper/eavesdropper/lib/python3.11/site-packages/llama_cpp/llama.py:1999\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1998\u001b[0m )\n\u001b[0;32m-> 1999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Nikita/linux/InnopolisUniversity/F24/PMLDL/Eavesdropper/eavesdropper/lib/python3.11/site-packages/llama_cpp/llama_chat_format.py:637\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    633\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    634\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    635\u001b[0m         )\n\u001b[0;32m--> 637\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Nikita/linux/InnopolisUniversity/F24/PMLDL/Eavesdropper/eavesdropper/lib/python3.11/site-packages/llama_cpp/llama.py:1833\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1833\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/Documents/Nikita/linux/InnopolisUniversity/F24/PMLDL/Eavesdropper/eavesdropper/lib/python3.11/site-packages/llama_cpp/llama.py:1269\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1271\u001b[0m     )\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (1618) exceed context window of 512"
     ]
    }
   ],
   "source": [
    "complition = llm.create_chat_completion(\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": final_prompt\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "answer = complition['choices'][0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eavesdropper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
