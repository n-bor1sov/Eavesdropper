{"cells":[{"cell_type":"markdown","metadata":{"id":"Tce3stUlHN0L"},"source":["##### Copyright 2024 Google LLC."]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","execution":{"iopub.execute_input":"2024-10-27T17:01:02.481383Z","iopub.status.busy":"2024-10-27T17:01:02.480968Z","iopub.status.idle":"2024-10-27T17:01:02.485817Z","shell.execute_reply":"2024-10-27T17:01:02.484890Z","shell.execute_reply.started":"2024-10-27T17:01:02.481345Z"},"id":"tuOe1ymfHZPu","trusted":true},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"aQXQaW_hv5RT"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/pytorch_gemma\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"PXNm5_p_oxMF"},"source":["# Gemma in PyTorch\n","\n","This is a quick demo of running Gemma inference in PyTorch.\n","For more details, please check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n","\n","**Note that**:\n"," * The free Kaggle CPU Python runtime and GPU Python runtime are sufficient for running the Gemma 2B models and 7B int8 quantized models.\n"," * For advanced use cases for other GPUs or TPU, please refer to [README.md](https://github.com/google/gemma_pytorch/blob/main/README.md) in the official repo."]},{"cell_type":"markdown","metadata":{"id":"jbza6uQdA-0P"},"source":["### Gemma setup\n","\n","To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n","\n","Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n","\n","- Sign in or register at [kaggle.com](https://www.kaggle.com)\n","- Open the [Gemma 2 model card](https://www.kaggle.com/models/google/gemma-2) and select _\"Request Access\"_\n","- Complete the consent form and accept the terms and conditions\n"]},{"cell_type":"markdown","metadata":{"id":"Fqq3HDVfA6Xm"},"source":["## Install dependencies"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:03.813109Z","iopub.status.busy":"2024-10-27T17:01:03.812379Z","iopub.status.idle":"2024-10-27T17:01:16.715204Z","shell.execute_reply":"2024-10-27T17:01:16.714041Z","shell.execute_reply.started":"2024-10-27T17:01:03.813070Z"},"id":"bMboT70Xop8G","outputId":"42a0d20f-8e74-4772-d9ff-1a52d4daf4bd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q -U torch==2.4.0 immutabledict sentencepiece"]},{"cell_type":"markdown","metadata":{"id":"ENdjDV3nBG5Z"},"source":["## Download model weights"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:16.717545Z","iopub.status.busy":"2024-10-27T17:01:16.717217Z","iopub.status.idle":"2024-10-27T17:01:16.722959Z","shell.execute_reply":"2024-10-27T17:01:16.721895Z","shell.execute_reply.started":"2024-10-27T17:01:16.717510Z"},"id":"GU5ZZzcZ6ik3","trusted":true},"outputs":[],"source":["# Choose variant and machine type\n","VARIANT = '2b-it' #@param ['2b', '2b-it', '9b', '9b-it', '27b', '27b-it']\n","MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']\n","\n","CONFIG = VARIANT[:2]\n","if CONFIG == '2b':\n","  CONFIG = '2b-v2'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:16.724439Z","iopub.status.busy":"2024-10-27T17:01:16.724162Z","iopub.status.idle":"2024-10-27T17:01:16.733610Z","shell.execute_reply":"2024-10-27T17:01:16.732691Z","shell.execute_reply.started":"2024-10-27T17:01:16.724408Z"},"trusted":true},"outputs":[],"source":["weights_dir = \"../models/gemma-2-pytorch-gemma-2-2b-it-v1/\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:16.735560Z","iopub.status.busy":"2024-10-27T17:01:16.735248Z","iopub.status.idle":"2024-10-27T17:01:16.749344Z","shell.execute_reply":"2024-10-27T17:01:16.748611Z","shell.execute_reply.started":"2024-10-27T17:01:16.735529Z"},"id":"viESUwjq5cAz","trusted":true},"outputs":[],"source":["import os\n","# Ensure that the tokenizer is present\n","tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n","assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n","\n","# Ensure that the checkpoint is present\n","ckpt_path = os.path.join(weights_dir, f'model.ckpt')\n","assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"]},{"cell_type":"markdown","metadata":{"id":"hOft88e7BOBB"},"source":["## Download the model implementation"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:23.369038Z","iopub.status.busy":"2024-10-27T17:01:23.368513Z","iopub.status.idle":"2024-10-27T17:01:26.413692Z","shell.execute_reply":"2024-10-27T17:01:26.412924Z","shell.execute_reply.started":"2024-10-27T17:01:23.368986Z"},"id":"sw-KBZ1vBSl3","trusted":true},"outputs":[],"source":["import sys\n","import torch\n","sys.path.append('../models/gemma-2-pytorch-gemma-2-2b-it-v1/pytorch/gemma_pytorch')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:26.415204Z","iopub.status.busy":"2024-10-27T17:01:26.414782Z","iopub.status.idle":"2024-10-27T17:01:26.443495Z","shell.execute_reply":"2024-10-27T17:01:26.442804Z","shell.execute_reply.started":"2024-10-27T17:01:26.415170Z"},"id":"XFUXlF74BTNe","trusted":true},"outputs":[],"source":["from gemma.config import GemmaConfig, get_model_config\n","from gemma.model import GemmaForCausalLM\n","from gemma.tokenizer import Tokenizer\n","import contextlib\n","import os\n"]},{"cell_type":"markdown","metadata":{"id":"-9PvhVSYBWBt"},"source":["## Setup the model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:26.446105Z","iopub.status.busy":"2024-10-27T17:01:26.445660Z","iopub.status.idle":"2024-10-27T17:01:48.130027Z","shell.execute_reply":"2024-10-27T17:01:48.129204Z","shell.execute_reply.started":"2024-10-27T17:01:26.446063Z"},"id":"e2olXB1b45Hz","trusted":true},"outputs":[],"source":["# Set up model config.\n","model_config = get_model_config(CONFIG)\n","model_config.tokenizer = tokenizer_path\n","model_config.quant = 'quant' in VARIANT\n","\n","# Instantiate the model and load the weights.\n","torch.set_default_dtype(model_config.get_dtype())\n","device = torch.device('cpu')\n","model = GemmaForCausalLM(model_config)\n","model.load_weights(ckpt_path)\n","model = model.to(device).eval()"]},{"cell_type":"markdown","metadata":{"id":"738CGmN-BocU"},"source":["## Run inference\n","\n","Below are examples for generating in chat mode and generating with multiple\n","requests.\n","\n","The instruction-tuned Gemma models were trained with a specific formatter that\n","annotates instruction tuning examples with extra information, both during\n","training and inference. The annotations (1) indicate roles in a conversation,\n","and (2) delineate turns in a conversation. Below we show a sample code snippet\n","for formatting the model prompt using the user and model chat templates in a\n","multi-turn conversation. The relevant tokens are:\n","\n","- `user`: user turn\n","- `model`: model turn\n","- `<start_of_turn>`: beginning of dialogue turn\n","- `<end_of_turn><eos>`: end of dialogue turn\n","\n","Read about the Gemma formatting for instruction tuning and system instructions\n","[here](https://ai.google.dev/gemma/docs/formatting)."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:01:48.131508Z","iopub.status.busy":"2024-10-27T17:01:48.131191Z","iopub.status.idle":"2024-10-27T17:01:54.935013Z","shell.execute_reply":"2024-10-27T17:01:54.934102Z","shell.execute_reply.started":"2024-10-27T17:01:48.131474Z"},"id":"yygIK9DEIldp","outputId":"1b3677ee-2b81-46f5-e784-9e3a6784ea01","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Chat prompt:\n"," <start_of_turn>user\n","What is a good place for travel in the US?<end_of_turn><eos>\n","<start_of_turn>model\n","California.<end_of_turn><eos>\n","<start_of_turn>user\n","What can I do in California?<end_of_turn><eos>\n","<start_of_turn>model\n","\n","That's a BIG question, California is HUGE! To help me narrow down the best options for YOU, tell me:\n","\n","* **What are you interested in?**  (Beaches, cities, nature, theme parks, history, culture, food, etc.) \n","* **Who are you traveling with?** (Solo, couple, family, friends)\n","* **How much time do you have?** (A weekend trip, a week, longer?)\n","* **What's your budget?** (Luxurious, budget-friendly, somewhere in between?)\n","\n","\n","The more specifics you give me, the better\n"]}],"source":["# Generate with one request in chat mode\n","\n","# Chat templates\n","USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n","MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n","\n","# Sample formatted prompt\n","prompt = (\n","    USER_CHAT_TEMPLATE.format(\n","        prompt='What is a good place for travel in the US?'\n","    )\n","    + MODEL_CHAT_TEMPLATE.format(prompt='California.')\n","    + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\n","    + '<start_of_turn>model\\n'\n",")\n","print('Chat prompt:\\n', prompt)\n","\n","results = model.generate(\n","    USER_CHAT_TEMPLATE.format(prompt=prompt),\n","    device=device,\n","    output_len=128,\n",")\n","print(results)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:35:14.955568Z","iopub.status.busy":"2024-10-27T17:35:14.955246Z","iopub.status.idle":"2024-10-27T17:35:14.970276Z","shell.execute_reply":"2024-10-27T17:35:14.969412Z","shell.execute_reply.started":"2024-10-27T17:35:14.955534Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\n[SPEAKER_00]: And today, as Jim proved to us, without talking about artificial intelligence and large language models, I typically say artificial intelligence is autocorrect on steroids because all a large language model does is it predicts what's the most likely next word that you're going to use and then it extrapolates from there.\\n\\n\\n[SPEAKER_00]: So not really very intelligent.\\n\\n\\n[SPEAKER_00]: Obviously, the impact that it has on our lives and on the reality we live in is significant.\\n\\n\\n[SPEAKER_00]: Do you think we will see LLM written code that is submitted to you as a progress?\\n\\n\\n[SPEAKER_01]: I'm convinced it's going to happen, yes.\\n\\n\\n[SPEAKER_01]: And it may well be happening already, maybe on a smaller scale where people really use it more as a help in writing code.\\n\\n\\n[SPEAKER_01]: It's clearly something where automation has always helped people write code.\\n\\n\\n[SPEAKER_01]: I mean, this is not anything new at all.\\n\\n\\n[SPEAKER_01]: We don't write machine code anymore.\\n\\n10\\n\\n[SPEAKER_01]: We don't even write assembler.\\n\\n11\\n\\n[SPEAKER_01]: And now we're moving on from C to Rust.\\n\\n12\\n\\n[SPEAKER_01]: So I don't see this as something\\n\\n13\\n\\n[SPEAKER_01]: as revolutionary as all the news talk is every day about AI.\\n\\n14\\n\\n[SPEAKER_01]: It's not an area I obviously work with.\\n\\n15\\n\\n[SPEAKER_01]: I'm still very low level.\\n\\n16\\n\\n[SPEAKER_01]: I got into kernels because I love the low level hardware details.\\n\\n17\\n\\n[SPEAKER_01]: why I'm still there.\\n\\n18\\n\\n[SPEAKER_00]: But so you say you expect this can help people write code, this can help people get started, but then if we look at the previous conversation and the challenges around code reviews and maintaining, so do you think that large language models will get to the point that they can help us review code, that they can help maintain subsistence?\\n\\n19\\n\\n[SPEAKER_01]: I hope, I hope because that's certainly one of the areas where\\n\\n20\\n\\n[SPEAKER_01]: which I see them really being able to shine to find the obvious stupid bugs, because I mean, how many of people here are actually programmers in this room?\\n\\n21\\n\\n[SPEAKER_01]: So a fair number.\\n\\n22\\n\\n[SPEAKER_01]: A lot of the bugs I've read, right, a lot of the bugs I see other people write, they're not like subtle bugs.\\n\\n23\\n\\n[SPEAKER_01]: A lot of them are just the stupid bugs that you did not think about, and you don't need\\n\\n24\\n\\n[SPEAKER_01]: any kind of higher intelligence to find them.\\n\\n25\\n\\n[SPEAKER_01]: But having tools that warn, I mean, we have compilers that warn about the really obvious ones, but having LLMs that warn about slightly more subtle cases where it may just say, this pattern does not look like the regular pattern.\\n\\n26\\n\\n[SPEAKER_01]: Are you sure this is what you mean?\\n\\n27\\n\\n[SPEAKER_01]: And the answer may be, no, that was not at all what I meant.\\n\\n28\\n\\n[SPEAKER_01]: You found an obvious bug.\\n\\n29\\n\\n[SPEAKER_01]: Thank you very much.\\n\\n30\\n\\n[SPEAKER_01]: So I do think that LLMs are going to be a big, you call them disparagingly, like auto-corrects on steroids.\\n\\n31\\n\\n[SPEAKER_01]: And I actually think that they're way more than that and how most people work is we all are.\\n\\n32\\n\\n[SPEAKER_01]: auto-corrects on steroids to some degree.\\n\\n33\\n\\n[SPEAKER_01]: And I see that as a tool that can help us be better at what we do.\\n\\n34\\n\\n[SPEAKER_01]: But I've always been optimistic.\\n\\n35\\n\\n[SPEAKER_01]: The whole... Hopeful.\\n\\n36\\n\\n[SPEAKER_01]: Hopeful was the word you had.\\n\\n37\\n\\n[SPEAKER_00]: Yes, yes.\\n\\n38\\n\\n[SPEAKER_00]: Helpful, hopeful and humble.\\n\\n39\\n\\n[SPEAKER_01]: Hopeful and humble, that's my middle name.\\n\\n40\\n\\n[SPEAKER_01]: But...\\n\\n41\\n\\n[SPEAKER_01]: On the other hand, I mean, I have been so optimistic that 32 years ago I was stupid enough to think that you can write a better kernel than anybody else.\\n\\n42\\n\\n[SPEAKER_01]: So you have to kind of be a bit too optimistic at times to make a difference.\\n\\n43\\n\\n[SPEAKER_01]: So my approach to OOMs really has to be that, hey, this is wonderful.\\n\\n44\\n\\n[SPEAKER_01]: This is going to\\n\\n45\\n\\n[SPEAKER_00]: I love seeing the optimism.\\n\\n46\\n\\n[SPEAKER_00]: I don't necessarily share it.\\n\\n47\\n\\n[SPEAKER_01]: No, a lot of people disagree with it.\\n\\n48\\n\\n[SPEAKER_00]: But one of the things that I worry about in all this is we see the hallucinations.\\n\\n49\\n\\n[SPEAKER_00]: We see, and that's a technical term for LLMs, they do hallucinate and they do makeup stuff.\\n\\n50\\n\\n[SPEAKER_00]: And so the more they are being put into the position where they will automatically do things without an actual human being there to catch them, the more this becomes scary.\\n\\n51\\n\\n[SPEAKER_00]: not as curious and they will rule the world and not in the sci-fi sense, but in the so many bugs that will happen and that will affect our lives or our code.\\n\\n52\\n\\n[SPEAKER_01]: Well, I see the bugs that happen without them every day, so that may be why I'm not so worried.\\n\\n53\\n\\n[SPEAKER_01]: I think we're doing those just fine on our own.\\n\\n54\\n\\n[SPEAKER_00]: And I don't think I can end the AI topic on a better highlight, so let's go from there to...\\n\\n\""]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","sample_path = \"/kaggle/input/video-transcribation-sample/Sample.txt\"\n","cleaned_text = ''\n","with open(sample_path, \"r\") as file:\n","    content = file.read()\n","    text_without_timestamps = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n","    cleaned_text = re.sub(r'^\\d$\\n', '', text_without_timestamps, flags=re.MULTILINE)\n","cleaned_text"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-27T17:36:16.952698Z","iopub.status.busy":"2024-10-27T17:36:16.952310Z","iopub.status.idle":"2024-10-27T17:36:30.570391Z","shell.execute_reply":"2024-10-27T17:36:30.569474Z","shell.execute_reply.started":"2024-10-27T17:36:16.952661Z"},"id":"oP746yI9PirY","outputId":"bdee9e0c-0bb8-48f1-da61-0cef8c036ac9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Online Meeting Summary & Insights Assistant\n","Task:\n","Analyze Discussion Structure: Identify and label key topics/subtopics discussed during the meeting.\n","Speaker-wise Subtopic Summaries: For each subtopic, provide a brief summary (1-2 sentences) of the key points mentioned by each relevant speaker.\n","Meeting Conclusion & Key Takeaways:\n","Short Conclusion (2-3 sentences): Outline the overall discussion and meeting outcome.\n","Emphasized Key Points (bullet points): Highlight the most crucial decisions, actions, or agreements from the meeting.\n","Input:\n","\n","Meeting Transcript: \n","\n","\n","[SPEAKER_00]: And today, as Jim proved to us, without talking about artificial intelligence and large language models, I typically say artificial intelligence is autocorrect on steroids because all a large language model does is it predicts what's the most likely next word that you're going to use and then it extrapolates from there.\n","[SPEAKER_00]: So not really very intelligent.\n","[SPEAKER_00]: Obviously, the impact that it has on our lives and on the reality we live in is significant.\n","[SPEAKER_00]: Do you think we will see LLM written code that is submitted to you as a progress?\n","[SPEAKER_01]: I'm convinced it's going to happen, yes.\n","[SPEAKER_01]: And it may well be happening already, maybe on a smaller scale where people really use it more as a help in writing code.\n","[SPEAKER_01]: It's clearly something where automation has always helped people write code.\n","[SPEAKER_01]: I mean, this is not anything new at all.\n","[SPEAKER_01]: We don't write machine code anymore.\n","[SPEAKER_01]: We don't even write assembler.\n","[SPEAKER_01]: And now we're moving on from C to Rust.\n","[SPEAKER_01]: So I don't see this as something\n","[SPEAKER_01]: as revolutionary as all the news talk is every day about AI.\n","[SPEAKER_01]: It's not an area I obviously work with.\n","[SPEAKER_01]: I'm still very low level.\n","[SPEAKER_01]: I got into kernels because I love the low level hardware details.\n","[SPEAKER_01]: why I'm still there.\n","[SPEAKER_00]: But so you say you expect this can help people write code, this can help people get started, but then if we look at the previous conversation and the challenges around code reviews and maintaining, so do you think that large language models will get to the point that they can help us review code, that they can help maintain subsistence?\n","[SPEAKER_01]: I hope, I hope because that's certainly one of the areas where\n","[SPEAKER_01]: which I see them really being able to shine to find the obvious stupid bugs, because I mean, how many of people here are actually programmers in this room?\n","[SPEAKER_01]: So a fair number.\n","[SPEAKER_01]: A lot of the bugs I've read, right, a lot of the bugs I see other people write, they're not like subtle bugs.\n","[SPEAKER_01]: A lot of them are just the stupid bugs that you did not think about, and you don't need\n","[SPEAKER_01]: any kind of higher intelligence to find them.\n","[SPEAKER_01]: But having tools that warn, I mean, we have compilers that warn about the really obvious ones, but having LLMs that warn about slightly more subtle cases where it may just say, this pattern does not look like the regular pattern.\n","[SPEAKER_01]: Are you sure this is what you mean?\n","[SPEAKER_01]: And the answer may be, no, that was not at all what I meant.\n","[SPEAKER_01]: You found an obvious bug.\n","[SPEAKER_01]: Thank you very much.\n","[SPEAKER_01]: So I do think that LLMs are going to be a big, you call them disparagingly, like auto-corrects on steroids.\n","[SPEAKER_01]: And I actually think that they're way more than that and how most people work is we all are.\n","[SPEAKER_01]: auto-corrects on steroids to some degree.\n","[SPEAKER_01]: And I see that as a tool that can help us be better at what we do.\n","[SPEAKER_01]: But I've always been optimistic.\n","[SPEAKER_01]: The whole... Hopeful.\n","[SPEAKER_01]: Hopeful was the word you had.\n","[SPEAKER_00]: Yes, yes.\n","[SPEAKER_00]: Helpful, hopeful and humble.\n","[SPEAKER_01]: Hopeful and humble, that's my middle name.\n","[SPEAKER_01]: But...\n","[SPEAKER_01]: On the other hand, I mean, I have been so optimistic that 32 years ago I was stupid enough to think that you can write a better kernel than anybody else.\n","[SPEAKER_01]: So you have to kind of be a bit too optimistic at times to make a difference.\n","[SPEAKER_01]: So my approach to OOMs really has to be that, hey, this is wonderful.\n","[SPEAKER_01]: This is going to\n","[SPEAKER_00]: I love seeing the optimism.\n","[SPEAKER_00]: I don't necessarily share it.\n","[SPEAKER_01]: No, a lot of people disagree with it.\n","[SPEAKER_00]: But one of the things that I worry about in all this is we see the hallucinations.\n","[SPEAKER_00]: We see, and that's a technical term for LLMs, they do hallucinate and they do makeup stuff.\n","[SPEAKER_00]: And so the more they are being put into the position where they will automatically do things without an actual human being there to catch them, the more this becomes scary.\n","[SPEAKER_00]: not as curious and they will rule the world and not in the sci-fi sense, but in the so many bugs that will happen and that will affect our lives or our code.\n","[SPEAKER_01]: Well, I see the bugs that happen without them every day, so that may be why I'm not so worried.\n","[SPEAKER_01]: I think we're doing those just fine on our own.\n","[SPEAKER_00]: And I don't think I can end the AI topic on a better highlight, so let's go from there to...\n","\n","\n","\n","Desired Output Format:\n","\n","**Topic 1: [ Brief Topic Description ]**\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","*...\n","\n","**Topic 2: [ Brief Topic Description ]**\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","*...\n","\n","**Conclusion:**\n","* Brief summary of the overall discussion and meeting outcome (2-3 sentences)\n","\n","**Key Takeaways:**\n","* • Crucial Decision/Action 1\n","* • Crucial Decision/Action 2\n","* •...\n","\n","Answer:\n","\n","```\n","#  Meeting Discussion Structure Analysis\n","## Topic 1:  Understanding LLMs and Their Impact\n","\n","* **Speaker 00**:  Defined AI as autocorrect on steroids, explained that LLMs predict the next most likely word. \n","* **Speaker 01**:  Encouraged optimistic positive view on LLMs,  explains they are useful tool to automate tasks and help programmers. \n","* **Speaker 01**:  Outlined how LLMs are replacing traditional programming styles by automating tasks, cited  C++, Rust, and other programming languages. \n","\n","\n","## Topic 2: Potential Applications of LLMs\n","\n","* **Speaker 00**:  Asked about the possibility of LLMs helping with code reviews, maintenance.\n","* **Speaker 01**:  Acknowledged LLMs have promise to find \"obvious stupid bugs\" and suggest more than auto-corrects on steroids, can even help find subtle bugs with more complexity.\n","*  **Speaker 00**: shared concerns about potential risks and hallucinations with LLMs becoming automatic code writers without human oversight. \n","\n","## Conclusion:\n","\n","The meeting discussed AI's impact on programming, specifically, LLMs and their potential for assisting in code review and maintenance. The debate revolved\n"]}],"source":["prompt = \"\"\"Online Meeting Summary & Insights Assistant\n","Task:\n","Analyze Discussion Structure: Identify and label key topics/subtopics discussed during the meeting.\n","Speaker-wise Subtopic Summaries: For each subtopic, provide a brief summary (1-2 sentences) of the key points mentioned by each relevant speaker.\n","Meeting Conclusion & Key Takeaways:\n","Short Conclusion (2-3 sentences): Outline the overall discussion and meeting outcome.\n","Emphasized Key Points (bullet points): Highlight the most crucial decisions, actions, or agreements from the meeting.\n","Input:\n","\n","Meeting Transcript: \n","\n","{script}\n","\n","Desired Output Format:\n","\n","**Topic 1: [ Brief Topic Description ]**\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","*...\n","\n","**Topic 2: [ Brief Topic Description ]**\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n","*...\n","\n","**Conclusion:**\n","* Brief summary of the overall discussion and meeting outcome (2-3 sentences)\n","\n","**Key Takeaways:**\n","* • Crucial Decision/Action 1\n","* • Crucial Decision/Action 2\n","* •...\n","\n","Answer:\"\"\"\n","def clean_text(input_text):\n","    \"\"\"\n","    Removes lines containing only numbers from the input text.\n","\n","    Args:\n","        input_text (str): The text to be cleaned.\n","\n","    Returns:\n","        str: The cleaned text with number-only lines removed.\n","    \"\"\"\n","    # Split the input text into lines\n","    lines = input_text.split('\\n')\n","    \n","    # Filter out lines that contain only numbers (possibly with leading/trailing whitespace)\n","    cleaned_lines = [line for line in lines if not re.match(r'^\\s*\\d+\\s*$', line)]\n","    \n","    # Join the cleaned lines back into a single string\n","    cleaned_text = '\\n'.join(cleaned_lines)\n","    \n","    return cleaned_text\n","cleaned_text = clean_text(cleaned_text).replace('\\n\\n\\n', '\\n')\n","# print(prompt.format(script=cleaned_text))\n","# Generate sample\n","results = model.generate(\n","    prompt.format(script=cleaned_text),\n","    device=device,\n","    output_len=256,\n",")\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"IF7B-3UJHMPd"},"source":["## Learn more\n","\n","Now that you have learned how to use Gemma in Pytorch, you can explore the many\n","other things that Gemma can do in [ai.google.dev/gemma](https://ai.google.dev/gemma).\n","See also these other related resources:\n","\n","- [Gemma model card](https://ai.google.dev/gemma/docs/model_card)\n","- [Gemma C++ Tutorial](https://ai.google.dev/gemma/docs/gemma_cpp)\n","- [Gemma formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Request successfully queued. Response: {'message': 'Request queued for processing'}\n"]}],"source":["import requests\n","import json\n","\n","def make_predict_request(prompt, url='http://localhost:5000/predict'):\n","    \"\"\"\n","    Makes a POST request to the /predict endpoint with a given prompt.\n","    \n","    Args:\n","    - prompt (str): The input prompt for the GEMMA model.\n","    - url (str, optional): The URL of the /predict endpoint. Defaults to 'http://localhost:5000/predict'.\n","    \n","    Returns:\n","    - response (requests.Response): The response from the server.\n","    \"\"\"\n","    # Prepare the request data\n","    data = {'prompt': prompt}\n","    \n","    # Convert the data to JSON\n","    json_data = json.dumps(data)\n","    \n","    # Set the appropriate headers (Content-Type for JSON)\n","    headers = {'Content-Type': 'application/json'}\n","    \n","    # Make the POST request\n","    response = requests.post(url, headers=headers, data=json_data)\n","    \n","    return response\n","\n","# Example usage\n","if __name__ == '__main__':\n","    prompt = \"Hello, how are you?\"  # Your input prompt here\n","    response = make_predict_request(prompt)\n","    \n","    # Check the response status code\n","    if response.status_code == 202:\n","        print(\"Request successfully queued. Response:\", response.json())\n","    else:\n","        print(\"Failed to queue request. Status code:\", response.status_code)\n","        print(\"Response content:\", response.text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5959217,"sourceId":9736631,"sourceType":"datasetVersion"},{"modelId":76277,"modelInstanceId":72240,"sourceId":85979,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"eavesdropper","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
