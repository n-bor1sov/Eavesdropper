{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:02.481383Z",
     "iopub.status.busy": "2024-10-27T17:01:02.480968Z",
     "iopub.status.idle": "2024-10-27T17:01:02.485817Z",
     "shell.execute_reply": "2024-10-27T17:01:02.484890Z",
     "shell.execute_reply.started": "2024-10-27T17:01:02.481345Z"
    },
    "id": "tuOe1ymfHZPu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQXQaW_hv5RT"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/pytorch_gemma\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXNm5_p_oxMF"
   },
   "source": [
    "# Gemma in PyTorch\n",
    "\n",
    "This is a quick demo of running Gemma inference in PyTorch.\n",
    "For more details, please check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n",
    "\n",
    "**Note that**:\n",
    " * The free Kaggle CPU Python runtime and GPU Python runtime are sufficient for running the Gemma 2B models and 7B int8 quantized models.\n",
    " * For advanced use cases for other GPUs or TPU, please refer to [README.md](https://github.com/google/gemma_pytorch/blob/main/README.md) in the official repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbza6uQdA-0P"
   },
   "source": [
    "### Gemma setup\n",
    "\n",
    "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
    "\n",
    "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
    "\n",
    "- Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
    "- Open the [Gemma 2 model card](https://www.kaggle.com/models/google/gemma-2) and select _\"Request Access\"_\n",
    "- Complete the consent form and accept the terms and conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqq3HDVfA6Xm"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:03.813109Z",
     "iopub.status.busy": "2024-10-27T17:01:03.812379Z",
     "iopub.status.idle": "2024-10-27T17:01:16.715204Z",
     "shell.execute_reply": "2024-10-27T17:01:16.714041Z",
     "shell.execute_reply.started": "2024-10-27T17:01:03.813070Z"
    },
    "id": "bMboT70Xop8G",
    "outputId": "42a0d20f-8e74-4772-d9ff-1a52d4daf4bd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U torch==2.4.0 immutabledict sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENdjDV3nBG5Z"
   },
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:16.717545Z",
     "iopub.status.busy": "2024-10-27T17:01:16.717217Z",
     "iopub.status.idle": "2024-10-27T17:01:16.722959Z",
     "shell.execute_reply": "2024-10-27T17:01:16.721895Z",
     "shell.execute_reply.started": "2024-10-27T17:01:16.717510Z"
    },
    "id": "GU5ZZzcZ6ik3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '2b-it' #@param ['2b', '2b-it', '9b', '9b-it', '27b', '27b-it']\n",
    "MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']\n",
    "\n",
    "CONFIG = VARIANT[:2]\n",
    "if CONFIG == '2b':\n",
    "  CONFIG = '2b-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:16.724439Z",
     "iopub.status.busy": "2024-10-27T17:01:16.724162Z",
     "iopub.status.idle": "2024-10-27T17:01:16.733610Z",
     "shell.execute_reply": "2024-10-27T17:01:16.732691Z",
     "shell.execute_reply.started": "2024-10-27T17:01:16.724408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights_dir = \"../models/gemma-2-pytorch-gemma-2-2b-it-v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:16.735560Z",
     "iopub.status.busy": "2024-10-27T17:01:16.735248Z",
     "iopub.status.idle": "2024-10-27T17:01:16.749344Z",
     "shell.execute_reply": "2024-10-27T17:01:16.748611Z",
     "shell.execute_reply.started": "2024-10-27T17:01:16.735529Z"
    },
    "id": "viESUwjq5cAz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Ensure that the tokenizer is present\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "# Ensure that the checkpoint is present\n",
    "ckpt_path = os.path.join(weights_dir, f'model.ckpt')\n",
    "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOft88e7BOBB"
   },
   "source": [
    "## Download the model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:23.369038Z",
     "iopub.status.busy": "2024-10-27T17:01:23.368513Z",
     "iopub.status.idle": "2024-10-27T17:01:26.413692Z",
     "shell.execute_reply": "2024-10-27T17:01:26.412924Z",
     "shell.execute_reply.started": "2024-10-27T17:01:23.368986Z"
    },
    "id": "sw-KBZ1vBSl3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../models/gemma-2-pytorch-gemma-2-2b-it-v1/pytorch/gemma_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:26.415204Z",
     "iopub.status.busy": "2024-10-27T17:01:26.414782Z",
     "iopub.status.idle": "2024-10-27T17:01:26.443495Z",
     "shell.execute_reply": "2024-10-27T17:01:26.442804Z",
     "shell.execute_reply.started": "2024-10-27T17:01:26.415170Z"
    },
    "id": "XFUXlF74BTNe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from gemma.config import GemmaConfig, get_model_config\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9PvhVSYBWBt"
   },
   "source": [
    "## Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:26.446105Z",
     "iopub.status.busy": "2024-10-27T17:01:26.445660Z",
     "iopub.status.idle": "2024-10-27T17:01:48.130027Z",
     "shell.execute_reply": "2024-10-27T17:01:48.129204Z",
     "shell.execute_reply.started": "2024-10-27T17:01:26.446063Z"
    },
    "id": "e2olXB1b45Hz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up model config.\n",
    "model_config = get_model_config(CONFIG)\n",
    "model_config.tokenizer = tokenizer_path\n",
    "model_config.quant = 'quant' in VARIANT\n",
    "\n",
    "# Instantiate the model and load the weights.\n",
    "torch.set_default_dtype(model_config.get_dtype())\n",
    "device = torch.device('cpu')\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "738CGmN-BocU"
   },
   "source": [
    "## Run inference\n",
    "\n",
    "Below are examples for generating in chat mode and generating with multiple\n",
    "requests.\n",
    "\n",
    "The instruction-tuned Gemma models were trained with a specific formatter that\n",
    "annotates instruction tuning examples with extra information, both during\n",
    "training and inference. The annotations (1) indicate roles in a conversation,\n",
    "and (2) delineate turns in a conversation. Below we show a sample code snippet\n",
    "for formatting the model prompt using the user and model chat templates in a\n",
    "multi-turn conversation. The relevant tokens are:\n",
    "\n",
    "- `user`: user turn\n",
    "- `model`: model turn\n",
    "- `<start_of_turn>`: beginning of dialogue turn\n",
    "- `<end_of_turn><eos>`: end of dialogue turn\n",
    "\n",
    "Read about the Gemma formatting for instruction tuning and system instructions\n",
    "[here](https://ai.google.dev/gemma/docs/formatting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:01:48.131508Z",
     "iopub.status.busy": "2024-10-27T17:01:48.131191Z",
     "iopub.status.idle": "2024-10-27T17:01:54.935013Z",
     "shell.execute_reply": "2024-10-27T17:01:54.934102Z",
     "shell.execute_reply.started": "2024-10-27T17:01:48.131474Z"
    },
    "id": "yygIK9DEIldp",
    "outputId": "1b3677ee-2b81-46f5-e784-9e3a6784ea01",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt:\n",
      " <start_of_turn>user\n",
      "What is a good place for travel in the US?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "California.<end_of_turn><eos>\n",
      "<start_of_turn>user\n",
      "What can I do in California?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "\n",
      "That's a BIG question, California is HUGE! To help me narrow down the best options for YOU, tell me:\n",
      "\n",
      "* **What are you interested in?**  (Beaches, cities, nature, theme parks, history, culture, food, etc.) \n",
      "* **Who are you traveling with?** (Solo, couple, family, friends)\n",
      "* **How much time do you have?** (A weekend trip, a week, longer?)\n",
      "* **What's your budget?** (Luxurious, budget-friendly, somewhere in between?)\n",
      "\n",
      "\n",
      "The more specifics you give me, the better\n"
     ]
    }
   ],
   "source": [
    "# Generate with one request in chat mode\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "\n",
    "# Sample formatted prompt\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt='What is a good place for travel in the US?'\n",
    "    )\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt='California.')\n",
    "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\n",
    "    + '<start_of_turn>model\\n'\n",
    ")\n",
    "print('Chat prompt:\\n', prompt)\n",
    "\n",
    "results = model.generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=128,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:35:14.955568Z",
     "iopub.status.busy": "2024-10-27T17:35:14.955246Z",
     "iopub.status.idle": "2024-10-27T17:35:14.970276Z",
     "shell.execute_reply": "2024-10-27T17:35:14.969412Z",
     "shell.execute_reply.started": "2024-10-27T17:35:14.955534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[SPEAKER_00]: And today, as Jim proved to us, without talking about artificial intelligence and large language models, I typically say artificial intelligence is autocorrect on steroids because all a large language model does is it predicts what's the most likely next word that you're going to use and then it extrapolates from there.\\n\\n\\n[SPEAKER_00]: So not really very intelligent.\\n\\n\\n[SPEAKER_00]: Obviously, the impact that it has on our lives and on the reality we live in is significant.\\n\\n\\n[SPEAKER_00]: Do you think we will see LLM written code that is submitted to you as a progress?\\n\\n\\n[SPEAKER_01]: I'm convinced it's going to happen, yes.\\n\\n\\n[SPEAKER_01]: And it may well be happening already, maybe on a smaller scale where people really use it more as a help in writing code.\\n\\n\\n[SPEAKER_01]: It's clearly something where automation has always helped people write code.\\n\\n\\n[SPEAKER_01]: I mean, this is not anything new at all.\\n\\n\\n[SPEAKER_01]: We don't write machine code anymore.\\n\\n10\\n\\n[SPEAKER_01]: We don't even write assembler.\\n\\n11\\n\\n[SPEAKER_01]: And now we're moving on from C to Rust.\\n\\n12\\n\\n[SPEAKER_01]: So I don't see this as something\\n\\n13\\n\\n[SPEAKER_01]: as revolutionary as all the news talk is every day about AI.\\n\\n14\\n\\n[SPEAKER_01]: It's not an area I obviously work with.\\n\\n15\\n\\n[SPEAKER_01]: I'm still very low level.\\n\\n16\\n\\n[SPEAKER_01]: I got into kernels because I love the low level hardware details.\\n\\n17\\n\\n[SPEAKER_01]: why I'm still there.\\n\\n18\\n\\n[SPEAKER_00]: But so you say you expect this can help people write code, this can help people get started, but then if we look at the previous conversation and the challenges around code reviews and maintaining, so do you think that large language models will get to the point that they can help us review code, that they can help maintain subsistence?\\n\\n19\\n\\n[SPEAKER_01]: I hope, I hope because that's certainly one of the areas where\\n\\n20\\n\\n[SPEAKER_01]: which I see them really being able to shine to find the obvious stupid bugs, because I mean, how many of people here are actually programmers in this room?\\n\\n21\\n\\n[SPEAKER_01]: So a fair number.\\n\\n22\\n\\n[SPEAKER_01]: A lot of the bugs I've read, right, a lot of the bugs I see other people write, they're not like subtle bugs.\\n\\n23\\n\\n[SPEAKER_01]: A lot of them are just the stupid bugs that you did not think about, and you don't need\\n\\n24\\n\\n[SPEAKER_01]: any kind of higher intelligence to find them.\\n\\n25\\n\\n[SPEAKER_01]: But having tools that warn, I mean, we have compilers that warn about the really obvious ones, but having LLMs that warn about slightly more subtle cases where it may just say, this pattern does not look like the regular pattern.\\n\\n26\\n\\n[SPEAKER_01]: Are you sure this is what you mean?\\n\\n27\\n\\n[SPEAKER_01]: And the answer may be, no, that was not at all what I meant.\\n\\n28\\n\\n[SPEAKER_01]: You found an obvious bug.\\n\\n29\\n\\n[SPEAKER_01]: Thank you very much.\\n\\n30\\n\\n[SPEAKER_01]: So I do think that LLMs are going to be a big, you call them disparagingly, like auto-corrects on steroids.\\n\\n31\\n\\n[SPEAKER_01]: And I actually think that they're way more than that and how most people work is we all are.\\n\\n32\\n\\n[SPEAKER_01]: auto-corrects on steroids to some degree.\\n\\n33\\n\\n[SPEAKER_01]: And I see that as a tool that can help us be better at what we do.\\n\\n34\\n\\n[SPEAKER_01]: But I've always been optimistic.\\n\\n35\\n\\n[SPEAKER_01]: The whole... Hopeful.\\n\\n36\\n\\n[SPEAKER_01]: Hopeful was the word you had.\\n\\n37\\n\\n[SPEAKER_00]: Yes, yes.\\n\\n38\\n\\n[SPEAKER_00]: Helpful, hopeful and humble.\\n\\n39\\n\\n[SPEAKER_01]: Hopeful and humble, that's my middle name.\\n\\n40\\n\\n[SPEAKER_01]: But...\\n\\n41\\n\\n[SPEAKER_01]: On the other hand, I mean, I have been so optimistic that 32 years ago I was stupid enough to think that you can write a better kernel than anybody else.\\n\\n42\\n\\n[SPEAKER_01]: So you have to kind of be a bit too optimistic at times to make a difference.\\n\\n43\\n\\n[SPEAKER_01]: So my approach to OOMs really has to be that, hey, this is wonderful.\\n\\n44\\n\\n[SPEAKER_01]: This is going to\\n\\n45\\n\\n[SPEAKER_00]: I love seeing the optimism.\\n\\n46\\n\\n[SPEAKER_00]: I don't necessarily share it.\\n\\n47\\n\\n[SPEAKER_01]: No, a lot of people disagree with it.\\n\\n48\\n\\n[SPEAKER_00]: But one of the things that I worry about in all this is we see the hallucinations.\\n\\n49\\n\\n[SPEAKER_00]: We see, and that's a technical term for LLMs, they do hallucinate and they do makeup stuff.\\n\\n50\\n\\n[SPEAKER_00]: And so the more they are being put into the position where they will automatically do things without an actual human being there to catch them, the more this becomes scary.\\n\\n51\\n\\n[SPEAKER_00]: not as curious and they will rule the world and not in the sci-fi sense, but in the so many bugs that will happen and that will affect our lives or our code.\\n\\n52\\n\\n[SPEAKER_01]: Well, I see the bugs that happen without them every day, so that may be why I'm not so worried.\\n\\n53\\n\\n[SPEAKER_01]: I think we're doing those just fine on our own.\\n\\n54\\n\\n[SPEAKER_00]: And I don't think I can end the AI topic on a better highlight, so let's go from there to...\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sample_path = \"/kaggle/input/video-transcribation-sample/Sample.txt\"\n",
    "cleaned_text = ''\n",
    "with open(sample_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "    text_without_timestamps = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n",
    "    cleaned_text = re.sub(r'^\\d$\\n', '', text_without_timestamps, flags=re.MULTILINE)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T17:36:16.952698Z",
     "iopub.status.busy": "2024-10-27T17:36:16.952310Z",
     "iopub.status.idle": "2024-10-27T17:36:30.570391Z",
     "shell.execute_reply": "2024-10-27T17:36:30.569474Z",
     "shell.execute_reply.started": "2024-10-27T17:36:16.952661Z"
    },
    "id": "oP746yI9PirY",
    "outputId": "bdee9e0c-0bb8-48f1-da61-0cef8c036ac9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Online Meeting Summary & Insights Assistant\n",
    "Task:\n",
    "Analyze Discussion Structure: Identify and label key topics/subtopics discussed during the meeting.\n",
    "Speaker-wise Subtopic Summaries: For each subtopic, provide a brief summary (1-2 sentences) of the key points mentioned by each relevant speaker.\n",
    "Meeting Conclusion & Key Takeaways:\n",
    "Short Conclusion (2-3 sentences): Outline the overall discussion and meeting outcome.\n",
    "Emphasized Key Points (bullet points): Highlight the most crucial decisions, actions, or agreements from the meeting.\n",
    "Input:\n",
    "\n",
    "Meeting Transcript: \n",
    "\n",
    "{script}\n",
    "\n",
    "Desired Output Format:\n",
    "\n",
    "**Topic 1: [ Brief Topic Description ]**\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "*...\n",
    "\n",
    "**Topic 2: [ Brief Topic Description ]**\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "* **Speaker [ID]**: [ Brief Summary of Speaker's Key Points (1-2 sentences) ]\n",
    "*...\n",
    "\n",
    "**Conclusion:**\n",
    "* Brief summary of the overall discussion and meeting outcome (2-3 sentences)\n",
    "\n",
    "**Key Takeaways:**\n",
    "* • Crucial Decision/Action 1\n",
    "* • Crucial Decision/Action 2\n",
    "* •...\n",
    "\n",
    "Answer:\"\"\"\n",
    "def clean_text(input_text):\n",
    "    \"\"\"\n",
    "    Removes lines containing only numbers from the input text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with number-only lines removed.\n",
    "    \"\"\"\n",
    "    # Split the input text into lines\n",
    "    lines = input_text.split('\\n')\n",
    "    \n",
    "    # Filter out lines that contain only numbers (possibly with leading/trailing whitespace)\n",
    "    cleaned_lines = [line for line in lines if not re.match(r'^\\s*\\d+\\s*$', line)]\n",
    "    \n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    return cleaned_text\n",
    "# cleaned_text = clean_text(cleaned_text).replace('\\n\\n\\n', '\\n')\n",
    "# # print(prompt.format(script=cleaned_text))\n",
    "# # Generate sample\n",
    "# results = model.generate(\n",
    "#     prompt.format(script=cleaned_text),\n",
    "#     device=device,\n",
    "#     output_len=256,\n",
    "# )\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF7B-3UJHMPd"
   },
   "source": [
    "## Learn more\n",
    "\n",
    "Now that you have learned how to use Gemma in Pytorch, you can explore the many\n",
    "other things that Gemma can do in [ai.google.dev/gemma](https://ai.google.dev/gemma).\n",
    "See also these other related resources:\n",
    "\n",
    "- [Gemma model card](https://ai.google.dev/gemma/docs/model_card)\n",
    "- [Gemma C++ Tutorial](https://ai.google.dev/gemma/docs/gemma_cpp)\n",
    "- [Gemma formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successfully queued. Response: {'message': 'Request queued for processing'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def make_predict_request(prompt, url='http://localhost:5000/predict'):\n",
    "    \"\"\"\n",
    "    Makes a POST request to the /predict endpoint with a given prompt.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input prompt for the GEMMA model.\n",
    "    - url (str, optional): The URL of the /predict endpoint. Defaults to 'http://localhost:5000/predict'.\n",
    "    \n",
    "    Returns:\n",
    "    - response (requests.Response): The response from the server.\n",
    "    \"\"\"\n",
    "    # Prepare the request data\n",
    "    data = {'prompt': prompt}\n",
    "    \n",
    "    # Convert the data to JSON\n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    # Set the appropriate headers (Content-Type for JSON)\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json_data)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    prompt = \"Hello, how are you?\"  # Your input prompt here\n",
    "    response = make_predict_request(prompt)\n",
    "    \n",
    "    # Check the response status code\n",
    "    if response.status_code == 202:\n",
    "        print(\"Request successfully queued. Response:\", response.json())\n",
    "    else:\n",
    "        print(\"Failed to queue request. Status code:\", response.status_code)\n",
    "        print(\"Response content:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from API:\n",
      "{\n",
      "    \"content\": \"## Meeting Summary & Insights\\n\\n**Topic 1:  The Impact of Large Language Models (LLMs)**\\n* **Speaker 00**:  LLMs are a form of auto-correction, predicting the most likely word to follow. While impactful, they are not truly intelligent.\\n* **Speaker 01**:  LLMs will likely help people write code, starting with simpler tasks. They can identify obvious bugs and offer suggestions for improvement. \\n* **Speaker 00**:  LLMs' potential for code review and maintenance is a concern. They can create errors and \\\"hallucinations\\\" that need human oversight.\\n\\n**Topic 2:  LLMs and Code Review**\\n* **Speaker 01**:  LLMs can help identify subtle bugs by comparing code patterns to expected ones.\\n* **Speaker 00**:  LLMs can be helpful tools, but their potential for error and \\\"hallucinations\\\" is a concern.\\n* **Speaker 01**:  LLMs can't replace human programmers, but they can assist in identifying and fixing bugs.\\n\\n\\n**Conclusion:** The meeting discussed the potential of LLMs in code writing and review. While LLMs can be helpful tools, there are concerns about their potential for errors and \\\"hallucinations.\\\"  \\n\\n**Key Takeaways:**\\n\\n* **LLMs will be a valuable tool for code writing and review.**\\n* **LLMs can identify subtle bugs and offer suggestions\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL of the Flask API\n",
    "API_URL = \"http://172.17.0.2:8000/chat\"\n",
    "\n",
    "\n",
    "import re\n",
    "sample_path = \"torvalds-speaks.txt\"\n",
    "cleaned_text = ''\n",
    "with open(sample_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "    text_without_timestamps = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', content)\n",
    "    cleaned_text = re.sub(r'^\\d$\\n', '', text_without_timestamps, flags=re.MULTILINE)\n",
    "# cleaned_text\n",
    "\n",
    "# Define the data payload for the POST request\n",
    "data = {\n",
    "    \"query\": cleaned_text\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(API_URL, json=data)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    result = response.json()\n",
    "    print(\"Response from API:\")\n",
    "    print(json.dumps(result, indent=4))\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from /home/nick/.cache/huggingface/hub/models--bartowski--gemma-2-2b-it-GGUF/snapshots/855f67caed130e1befc571b52bd181be2e858883/./gemma-2-2b-it-IQ4_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 30\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_loader: - type iq4_xs:  182 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 249\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = IQ4_XS - 4.25 bpw\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 1.45 GiB (4.77 BPW) \n",
      "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1487.93 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   208.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  208.00 MiB, K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '30', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-IQ4_XS.gguf\",\n",
    "    n_ctx=2048  # Increase the context window size\n",
    ")\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from /home/nick/.cache/huggingface/hub/models--bartowski--gemma-2-2b-it-GGUF/snapshots/855f67caed130e1befc571b52bd181be2e858883/gemma-2-2b-it-IQ4_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 30\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_loader: - type iq4_xs:  182 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 249\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = IQ4_XS - 4.25 bpw\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 1.45 GiB (4.77 BPW) \n",
      "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1487.93 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   208.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  208.00 MiB, K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '30', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "local_model_path = \"/home/nick/.cache/huggingface/hub/models--bartowski--gemma-2-2b-it-GGUF/snapshots/855f67caed130e1befc571b52bd181be2e858883/gemma-2-2b-it-IQ4_XS.gguf\"\n",
    "llm_1 = Llama(\n",
    "    model_path=local_model_path,\n",
    "    n_ctx=2048  # Set the desired context window size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   58228.65 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1748 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   84084.54 ms /  2047 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-81021aaa-956a-4448-8f1a-6cf464b75cf8',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1732141555,\n",
       " 'model': '/home/nick/.cache/huggingface/hub/models--bartowski--gemma-2-2b-it-GGUF/snapshots/855f67caed130e1befc571b52bd181be2e858883/gemma-2-2b-it-IQ4_XS.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Here\\'s a breakdown of the meeting discussion, incorporating the requested format: \\n\\n**Topic 1:  LLMs and Code Writing**\\n* **Speaker 00**:  Highlights the current state of LLMs, comparing them to autocorrect. States their concern about their potential for creating bugs or issues.\\n* **Speaker 01**: Expresses optimism that LLMs can aid in code writing and bug detection. Uses examples of compilers and \"obvious bugs\" to support their point.  They see LLMs as tools, not replacements for humans.\\n\\n**Topic 2: The Potential of LLMs in Code Review and Maintenance**\\n* **Speaker 00**:  Concerns about the risks associated with LLMs\\' lack of human oversight in code development. Uses terms like \"hallucinations\" and \"automatic code creation\" to underscore their potential dangers.\\n* **Speaker 01**:  Acknowledges some potential issues but emphasizes that they believe LLMs can be a valuable tool to identify and fix bugs. Expresses that they don\\'t think these issues are insurmountable.\\n\\n**Topic 3: The Role of Human Oversight and Optimism**\\n* **Speaker 00**:  Summarizes the main point of the discussion by stating that humans need to continue to carefully examine the outputs of LLMs and that \"hopeful and humble\" is not a suitable approach to the development of AI. \\n* **Speaker 01**:  Refirms'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 1748,\n",
       "  'completion_tokens': 300,\n",
       "  'total_tokens': 2048}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_1.create_chat_completion(\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": prompt.format(script=cleaned_text)\n",
    "\t\t}\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5959217,
     "sourceId": 9736631,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72240,
     "sourceId": 85979,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "eavesdropper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
