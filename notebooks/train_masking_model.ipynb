{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "#!pip install datasets"
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-27T11:57:23.903304Z",
     "start_time": "2024-10-27T11:57:23.898366Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ner.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T11:57:24.073882Z",
     "start_time": "2024-10-27T11:57:23.905696Z"
    }
   },
   "id": "83d1ff9492e347ef",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  Thousands of demonstrators have marched throug...   \n1  Iranian officials say they expect to get acces...   \n2  Helicopter gunships Saturday pounded militant ...   \n3  They left after a tense hour-long standoff wit...   \n4  U.N. relief coordinator Jan Egeland said Sunda...   \n\n                                              labels  \n0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n1  B-gpe O O O O O O O O O O O O O O B-tim O O O ...  \n2  O O B-tim O O O O O B-geo O O O O O B-org O O ...  \n3                              O O O O O O O O O O O  \n4  B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands of demonstrators have marched throug...</td>\n      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Iranian officials say they expect to get acces...</td>\n      <td>B-gpe O O O O O O O O O O O O O O B-tim O O O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Helicopter gunships Saturday pounded militant ...</td>\n      <td>O O B-tim O O O O O B-geo O O O O O B-org O O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>They left after a tense hour-long standoff wit...</td>\n      <td>O O O O O O O O O O O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n      <td>B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T11:57:24.104973Z",
     "start_time": "2024-10-27T11:57:24.076820Z"
    }
   },
   "id": "9a5f1cab2439747b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.labels = df.labels.str.replace('B-tim', 'O').str.replace('I-tim', 'O').str.replace('B-art','O').str.replace('I-art', 'O').str.replace('B-nat', 'O').str.replace('I-nat','O')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T11:57:24.212638Z",
     "start_time": "2024-10-27T11:57:24.106211Z"
    }
   },
   "id": "f61563aa6646b872",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Use the fast tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "texts = df['text'].tolist()  # Text sequences\n",
    "labels_text = df['labels'].tolist()  # Corresponding labels\n",
    "texts_split = [text.split() for text in texts]  # Tokenized text into words\n",
    "\n",
    "# Create a set of unique labels and map them to indices\n",
    "set_labels = set([label for label_seq in labels_text for label in label_seq.split()])\n",
    "num_labels = len(set_labels)\n",
    "\n",
    "dict_labels = {label: idx for idx, label in enumerate(set_labels)}  # Label -> index mapping\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(texts, labels, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(texts, max_length=128, is_split_into_words=True, truncation=True, padding='max_length')\n",
    "    word_ids = tokenized_inputs.word_ids()  # Get the word indices\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    # Align the labels with the tokenized inputs\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(0)  # Ignore subword tokens\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(dict_labels[labels[word_idx]])  # Assign label to the first wordpiece\n",
    "        else:\n",
    "            label_ids.append(dict_labels[labels[word_idx]] if label_all_tokens else 0)  # Option to propagate label to subwords\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Initialize lists to collect tokenized inputs\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "labels = []\n",
    "\n",
    "# Tokenize each sample and align its labels\n",
    "for i in range(len(texts_split)):\n",
    "    out = tokenize_and_align_labels(texts_split[i], labels_text[i].split())\n",
    "    input_ids.append(out['input_ids'])\n",
    "    attention_mask.append(out['attention_mask'])\n",
    "    labels.append(out['labels'])\n",
    "\n",
    "# Convert the merged tokenized inputs into a Dataset object\n",
    "dataset_dict = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    'labels': labels\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T11:58:09.287401Z",
     "start_time": "2024-10-27T11:57:24.214459Z"
    }
   },
   "id": "439d43023889a87a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\amirc\\anaconda3\\lib\\site-packages (4.46.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\amirc\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
      "Requirement already satisfied: psutil in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)env: CLEARML_API_HOST=https://api.clear.ml\n",
      "\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from accelerate) (2.2.2+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
      "Requirement already satisfied: sympy in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\amirc\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "env: CLEARML_API_ACCESS_KEY=GK5JXGH63PID8Q1HJM6LG0TDO32KEP\n",
      "env: CLEARML_API_SECRET_KEY=CpyJCNKsOPWU13ypw_uIk_f84U5Lax8ntgeHbm16s61dpNA1LIoUNfCFSKW6hz_VGm4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\amirc\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning:\n",
      "\n",
      "`evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-27 15:03:52,491 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers_3/accelerator_config[AcceleratorConfig])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirc\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2847: UserWarning:\n",
      "\n",
      "`max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AdamW' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 55\u001B[0m\n\u001B[0;32m     46\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     47\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,                         \u001B[38;5;66;03m# The model\u001B[39;00m\n\u001B[0;32m     48\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,                  \u001B[38;5;66;03m# Training arguments\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     51\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mdata_collator          \u001B[38;5;66;03m# Data collator for padding and aligning\u001B[39;00m\n\u001B[0;32m     52\u001B[0m )\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model(output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../models\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Save model to specified directory\u001B[39;00m\n\u001B[0;32m     57\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../models\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2054\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2388\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2385\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2387\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2388\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2391\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2392\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2393\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2394\u001B[0m ):\n\u001B[0;32m   2395\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2396\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3477\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   3475\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m   3476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mtrain):\n\u001B[1;32m-> 3477\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3479\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_inputs(inputs)\n\u001B[0;32m   3480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_sagemaker_mp_enabled():\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\accelerate\\optimizer.py:128\u001B[0m, in \u001B[0;36mAcceleratedOptimizer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    125\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;124;03m    Sets the optimizer to \"train\" mode. Useful for optimizers like `schedule_free`\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'AdamW' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=GK5JXGH63PID8Q1HJM6LG0TDO32KEP\n",
    "%env CLEARML_API_SECRET_KEY=CpyJCNKsOPWU13ypw_uIk_f84U5Lax8ntgeHbm16s61dpNA1LIoUNfCFSKW6hz_VGm4\n",
    "# End the previous run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Load pre-trained BERT model with classification head\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,              # Pass the tokenizer\n",
    "    padding=True,                     # Enable dynamic padding\n",
    "    max_length=128,                   # Set max length\n",
    "    label_pad_token_id=0          # Use -100 to ignore padding labels\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',               # Output directory for model predictions and checkpoints\n",
    "    num_train_epochs=3,                   # Total number of training epochs\n",
    "    per_device_train_batch_size=16,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,        # Batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate every epoch\n",
    "    save_steps=10_000,                    # Save checkpoint every 10,000 steps\n",
    "    save_total_limit=2,                   # Limit the total number of checkpoints\n",
    "    logging_dir='./logs',                 # Directory for storing logs\n",
    "    learning_rate=2e-5,                   # Learning rate\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator          # Data collator for padding and aligning\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir='../models')  # Save model to specified directory\n",
    "tokenizer.save_pretrained('../models')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T12:03:57.489788Z",
     "start_time": "2024-10-27T12:03:40.944920Z"
    }
   },
   "id": "6553b64ffb4b0ec6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Flatten the predictions and labels for calculating metrics\n",
    "    pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Only consider non-padding labels\n",
    "    non_pad_indices = labels_flat != -100\n",
    "    pred_flat = pred_flat[non_pad_indices]\n",
    "    labels_flat = labels_flat[non_pad_indices]\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(labels_flat, pred_flat)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Add the compute_metrics function to the Trainer\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Optionally, log evaluation results to MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "    mlflow.log_metrics(eval_results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-27T11:58:36.929936Z"
    }
   },
   "id": "f33183e66d5f2d7a",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
