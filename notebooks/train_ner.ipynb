{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Map: {'B-eve': 0, 'B-geo': 1, 'B-gpe': 2, 'B-org': 3, 'B-per': 4, 'I-eve': 5, 'I-geo': 6, 'I-gpe': 7, 'I-org': 8, 'I-per': 9, 'None': 10, 'O': 11}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Function to load NER data from a file\n",
    "def load_ner_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence, label = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, tag = line.split()\n",
    "                sentence.append(word)\n",
    "                label.append(tag)\n",
    "            else:\n",
    "                if sentence:  # End of a sentence\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "        # Catch the last sentence if there's no newline at the end\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "# Load your NER data\n",
    "train_sentences, train_labels = load_ner_data('../data/train_dataset/train.txt')\n",
    "valid_sentences, valid_labels = load_ner_data('../data/train_dataset/valid.txt')\n",
    "test_sentences, test_labels = load_ner_data('../data/train_dataset/test.txt')\n",
    "\n",
    "# Collect all unique labels from all datasets\n",
    "all_labels = set()\n",
    "for label_list in train_labels + valid_labels + test_labels:\n",
    "    all_labels.update(label_list)\n",
    "\n",
    "# Create label mapping\n",
    "label_map = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "print(\"Label Map:\", label_map)\n",
    "\n",
    "# Inverse mapping for decoding (if needed later)\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Encode labels as integers\n",
    "def encode_labels(labels):\n",
    "    return [[label_map[label] for label in sentence_labels] for sentence_labels in labels]\n",
    "\n",
    "train_labels_encoded = encode_labels(train_labels)\n",
    "valid_labels_encoded = encode_labels(valid_labels)\n",
    "test_labels_encoded = encode_labels(test_labels)\n",
    "\n",
    "# Convert to Dataset objects\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_sentences, \"ner_tags\": train_labels_encoded})\n",
    "valid_dataset = Dataset.from_dict({\"tokens\": valid_sentences, \"ner_tags\": valid_labels_encoded})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": test_sentences, \"ner_tags\": test_labels_encoded})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T14:59:40.399272Z",
     "start_time": "2024-10-27T14:59:37.407770Z"
    }
   },
   "id": "5ad47563502a617b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirc\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/33560 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c335385c5a3477dae10a83cc2f31e82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7193 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41605496e04946d48234851a1a6876fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7193 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "977894e077154c05a1c17f0944afa08d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\"  # Return tensors directly\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # Start with -100 for all tokens\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        for word_idx in range(len(label)):\n",
    "            if word_ids[word_idx] is not None:  # Only map if the word index is valid\n",
    "                label_ids[word_idx] = label[word_idx]  # Assign the correct label to the token index\n",
    "\n",
    "        # Ensure that label_ids are the same length as tokenized inputs\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Ensure that the labels are padded to the same length as input IDs\n",
    "    max_length = max(len(l) for l in labels)\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] += [-100] * (max_length - len(labels[i]))  # Pad with -100\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T14:59:53.013747Z",
     "start_time": "2024-10-27T14:59:40.400455Z"
    }
   },
   "id": "55eacb9bf799e946",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirc\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "num_labels = len(set(tag for tags in train_labels for tag in tags))  # Number of unique labels\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T14:59:58.568104Z",
     "start_time": "2024-10-27T14:59:53.014748Z"
    }
   },
   "id": "95581e73eefb0fc2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirc\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='6294' max='6294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6294/6294 1:11:54, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.160200</td>\n      <td>0.133460</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.115600</td>\n      <td>0.107394</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.096400</td>\n      <td>0.098014</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=6294, training_loss=0.1481890773864106, metrics={'train_runtime': 4315.3962, 'train_samples_per_second': 23.33, 'train_steps_per_second': 1.458, 'total_flos': 6577433975070720.0, 'train_loss': 0.1481890773864106, 'epoch': 3.0})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # Output directory\n",
    "    evaluation_strategy='epoch',      # Evaluation strategy\n",
    "    learning_rate=2e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Training batch size\n",
    "    per_device_eval_batch_size=16,    # Evaluation batch size\n",
    "    num_train_epochs=3,               # Total number of training epochs\n",
    "    weight_decay=0.01,                # Strength of weight decay\n",
    "    save_total_limit=2,               # Limit the total amount of checkpoints\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                       # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                # Training arguments, defined above\n",
    "    train_dataset=train_dataset,       # Training dataset\n",
    "    eval_dataset=valid_dataset         # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:55.413817Z",
     "start_time": "2024-10-27T14:59:58.570106Z"
    }
   },
   "id": "e6e0305d925ef583",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:55.444776Z",
     "start_time": "2024-10-27T16:11:55.422432Z"
    }
   },
   "id": "ea0a3a8c9c93c7d1",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 01:34]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.09713272005319595,\n 'eval_runtime': 94.8473,\n 'eval_samples_per_second': 75.838,\n 'eval_steps_per_second': 4.744,\n 'epoch': 3.0}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate(test_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:13:30.308364Z",
     "start_time": "2024-10-27T16:11:55.447058Z"
    }
   },
   "id": "ee7fd73d64f0cade",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "('../models/ner\\\\tokenizer_config.json',\n '../models/ner\\\\special_tokens_map.json',\n '../models/ner\\\\vocab.txt',\n '../models/ner\\\\added_tokens.json',\n '../models/ner\\\\tokenizer.json')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../models/ner')\n",
    "tokenizer.save_pretrained('../models/ner')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:13:30.774738Z",
     "start_time": "2024-10-27T16:13:30.309987Z"
    }
   },
   "id": "f900aaefcf55f2d6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]      O\n",
      "Hello      O\n",
      ",          O\n",
      "my         O\n",
      "name       O\n",
      "is         B-per\n",
      "Amir       O\n",
      ".          O\n",
      "I          O\n",
      "work       O\n",
      "at         B-org\n",
      "Open       O\n",
      "##A        O\n",
      "##I        O\n",
      "as         O\n",
      "a          O\n",
      "senior     O\n",
      "M          O\n",
      "##L        O\n",
      "engineer   O\n",
      ".          O\n",
      "[SEP]      O\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, my name is Amir. I work at OpenAI as a senior ML engineer.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", is_split_into_words=False, padding=True, truncation=True)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Run inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_token_class_ids = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "\n",
    "# Convert the predicted class IDs back to NER labels\n",
    "predicted_labels = [inverse_label_map[label_id] for label_id in predicted_token_class_ids]\n",
    "\n",
    "# Combine tokens with their corresponding labels for readability\n",
    "results = [(token, label) for token, label in zip(tokens, predicted_labels)]\n",
    "\n",
    "# Print the results\n",
    "for token, label in results:\n",
    "    print(f\"{token:10} {label}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:42:38.197106Z",
     "start_time": "2024-10-27T16:42:38.101988Z"
    }
   },
   "id": "f4356f69e3b4a855",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'B-eve',\n 1: 'B-geo',\n 2: 'B-gpe',\n 3: 'B-org',\n 4: 'B-per',\n 5: 'I-eve',\n 6: 'I-geo',\n 7: 'I-gpe',\n 8: 'I-org',\n 9: 'I-per',\n 10: 'None',\n 11: 'O'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_label_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:21:48.460549Z",
     "start_time": "2024-10-27T16:21:48.452441Z"
    }
   },
   "id": "d5b71862837379ff",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../models/label_map.pickle', 'wb') as f:\n",
    "    pickle.dump(inverse_label_map,f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T16:39:34.574270Z",
     "start_time": "2024-10-27T16:39:34.544160Z"
    }
   },
   "id": "fc1f94b3a8523bcf",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e0dfe8dcd266a194"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
